%
% File eacl2014.tex
%
% Contact g.bouma@rug.nl yannick.parmentier@univ-orleans.fr
%
% Based on the instruction file for ACL 2013 
% which in turns was based on the instruction files for previous 
% ACL and EACL conferences

%% Based on the instruction file for EACL 2006 by Eneko Agirre and Sergi Balari
%% and that of ACL 2008 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to
\usepackage{fancyvrb}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{array}
\usepackage{multirow}
\usepackage{CJK}
%\usepackage{bera}

\title{An Improved Semantic Role Labeling System for Chinese}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
    Third Author \\
    Affiliation / Address line 1 \\
    Affiliation / Address line 2 \\
    Affiliation / Address line 3 \\
    {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present an improved semantic role labeling system for Chinese parsed sentences. Our new system outperformed the previous reported one from two aspects: knowledge acquisition\{utilization\} and model design. As to the former, the semantic knowledge obtained from E-Hownet were utilized to solve the data sparseness issue; as to the latter, a combination of back-off models was proposed for semantic role classification. They enhanced the performance by 22.45\% and 77.55\%, respectively. Further performance gains through post-processing lead to an overall accuracy improvement from 92.71\% to 94.73\%.           
\end{abstract}
\section{Introduction}
Over the last few years, syntactic parsing paradigms have enjoyed an admirable success, and this has paved the way for semantic parsing. Semantic role labeling (SRL), also known as shallow semantic parsing, is the task of semantically annotating natural language text. Conventionally, a syntactically parsed sentence is taken as input, and semantic arguments associated with predicate of the sentence are identified and classified to a particular semantic class. \cite{Gildea:2002} were the first one to build an automatic SRL system, and since then, their ideas have been dominating the field. In their approach, they emphasized on selection of appropriate lexical and syntactical features for SRL, use of statistical classifiers and their combinations, and ways to handle the data sparseness issue. People have tried to build on that by augmenting and/or altering the feature set \cite{Chen:2003:UDL:1119355.1119361,Xue04calibratingfeatures}, by experimenting with various classification approaches \cite{Park:2005:MEB:1706543.1706583,tan-wang-2009}, and by trying different ways to handle data sparseness concern \cite{Zapirain:2007:USS:1621474.1621551,Lin:2010:CSR:1909632.1912231}. \\
In this study, we follow the traditions to enhance performance of an already reported SRL system for Chinese \cite{you-chen:2004} by (1) enriching its feature set (2) using a semantic knowledge-base to address the data sparseness. However, we take a step further, and propose a different classification approach that is based on a combination of weighted simple probabilistic models. To show the effectiveness of our idea, we build a number of systems that are based on other well established classification approaches (e.g. NaiveBayes, Decision Trees, Maximum Entropy, Linear Interpolation), and compare the outcomes. The experimental results show that our strategy outperformed all other systems, and lead to a considerable improvement in accuracy of the previously reported system.
\section{Experimental Material}
\subsection{Sinica Treebank}
Sinica treebank \cite{sinica-treebank} is a semantically annotated treebank for Chinese. Its version 3.0 contains 61,087 trees and 361,834 Chinese words. In addition to semantic relations of a verbal predicate, phrasal head-modifier relations have also been marked. There are 74 semantic roles including primary thematic roles (e.g. 'agent', 'theme', etc), secondary roles (e.g. 'location', 'time', 'place', etc), and noun-modification roles (e.g. 'quantifier', 'possessor' etc.). In this study, we have used this treebank as our training and testing data.     
\subsection{E-HowNet}
E-HowNet\cite{keylist} is a semantic lexical knowledge-base for Chinese. It is an entity-relation model that defines relationships between groups of words ("entities") based on their semantic attributes. The grouping, which is based on sense similarity, can be used to generalize lexical statistics. In this study, we have used E-HowNet to abate the effects of data sparseness on SRL by generalizing lexical statistics.  
%\section{Previous System}
%A semantic role labeling (SRL) system for Chinese was reported by \cite{you-chen:2004}. Input to the system was a parse tree, parsed by Sinica Chinese parser\footnote{An online demo of the parser is available at http://parser.iis.sinica.edu.tw/}, and output was its semantically labeled counterpart. Sinica Treebank \cite{sinica-treebank}, a semantically annotated Chinese treebank, was used to train simple probabilistic models, which were used for semantic role classification. As far feature set, the system relied on a number of conventional lexical (e.g. \verb+head_word+, \verb+head_word_pos+, \verb+target_word+, \verb+target_word_pos+) as well as syntactic features (e.g. \verb+phrase_type+, \verb+position+). The overall accuracy of the system was reported to be 92.71\%.
\section{Previous SRL System}
A semantic role labeling (SRL) system for Chinese was reported by \cite{you-chen:2004}. Sinica Treebank was used to train simple probabilistic models, which, together with a back-off strategy, were used for semantic role classification. As far feature set, the system relied on the following conventional lexical and syntactic features:\\[0.2cm]
\textbf{Lexical Features:}\\
\verb+h_word:+ Head word\\ 
\verb+h_pos:+ Part of speech tag of the head word\\ 
\verb+t_word:+ Target word\\ 
\verb+t_pos:+ Part of speech tag of the target word\\[0.2cm]
\textbf{Syntactic Features:} \\ \verb+pt:+ Phrase type\\ 
\verb+position:+ Position of the target word w.r.t verb  \\[0.2cm]
The overall accuracy of the system was reported to be 92.71\%. Even though this accuracy can be considered on the higher side, there was room for improvement, especially in two subtasks: data sparseness handling and classification approach. We have considerably improved the performance of the system by better addressing the data sparseness issue (Section 4.1), and by proposing a different classification approach (Section 4.3). 
%Data sparseness handling first, the old system used a back-off strategy to tackle this problem (see \cite{you-chen:2004} for more details). This considerably improved the baseline performance, but data sparseness still remained an issue. A major reason was dependency of the system on non-generalized lexical features (e.g. \verb+head_word+, \verb+target_word+). 
%As \cite{Gildea:2002} pointed out that lexical features, though very useful for semantic role labeling, may pose the issue of data sparseness. To handle this, they proposed a number of methods to generalize the lexical statistics including the use of a semantic hierarchy -- WordNet. However, they restricted their approach to the generalization of the head words of noun phrases only. Later, \cite{Lin:2010:CSR:1909632.1912231} experimented by extending it other lexical features, such as first word, last word etc, and witnessed improvements. 
%In our revised system, we have used E-HowNet to generalize two lexical features (i.e. \verb+head_word+ and \verb+target_word+), and hence, to better address the data sparseness concern.\\ 
%Classification methodology next. In the old system, simple probabilistic models, together with a back-off strategy, were used for semantic role assignment. As shown below, a particular feature combination was used for each model, and the backing-off was based on number of examples seen in the training data.
%\begin{verbatim}
%if#of(h,h_pos,t,t_pos,pt,position)
%                       >threshold
% P(r|constituent)=
% P(r|h,h_pos,t,t_pos,pt,position)
%else
% if#of(h_pos,t,t_pos,pt,position)
%                      >threshold
%  P(r|constituent)=
%  P(r|h_pos,t,t_pos,pt,position)
%else
%......
%\end{verbatim}
%A critical observation is that backing-off based on a constant threshold value did not allow to exploit the statistical knowledge in the best possible way. The reason is that if enough examples, with a more constrained feature combination, have been seen in the training data, it did not allow to utilize the less constrained statistical information, which may suggest a more probable role (see Section 4.3 for an example). We propose a different strategy that is based on a combination of weighted simple probabilistic models. In our method, both more and less constrained probabilities are ranked by learned weights, and then the top ranked information is used for final decision making.
\section{Revised SRL System}
\subsection{Feature Set}
\textbf{Lexical Features:} \\ 
\verb+h_pos:+ Part of speech tag of the head word\\ 
\verb+t_pos:+ Part of speech tag of the target word\\
\verb+left_right_child_pos:+ Part of speech tags of the immediate left and right siblings of a test node\\
\verb+all_pos:+ A set of part of speech tags of all nodes under a test node including the test node itself\\[0.2cm]
\textbf{Syntactic Features:} \\ 
\verb+pt:+ phrase type\\
\verb+passive:+ A sentence-level boolean feature indicating weather the sentence, containing the test node, is passive or not\\
\verb+position:+ Position of the target word w.r.t verb\\[0.2cm]
\textbf{Semantic Features:}\\
\verb+semType_h_pos:+ Semantic type of the head word extracted from E-HowNet \\ 
\verb+semType_t_pos:+ Semantic type of the target word extracted from E-HowNet\\ 
\verb+all_semType:+ A set of semantic types of all nodes of the tree, the test node is a child of\\[0.2cm]
As pointed out by \cite{Gildea:2002}, lexical statistics, though very useful for semantic role labeling, often becomes a source of data sparseness. %This is because, for a particular test case, the lexical values may not have been seen in the training data due to a large vocabulary size. 
%As an example, take two test cases with the feature set ($h\_word$, $h\_word\_pos$, $t\_word$, $t\_word\_pos$, $pt$, $position$) and with the following corresponding values:
%\begin{verbatim}
%(a) ...,N,...,N,NP,1
%(b) ...,N,...,N,NP,1
%\end{verbatim}
%Considering that, the particular head and target words in (a) have been, and in (b) have not been seen in the training data, the system will assign a semantic role to candidate (a), while failing to assign a role to candidate (b) unless we use some back-off or any other data sparseness handling strategy. One such strategy is generalization of the lexical features. 
For this reason, we have generalized two lexical features (i.e. \verb+h_word+, and \verb+t_word+) by replacing them with more general features (i.e. \verb+semType_h_word+ and \verb+semType_t_word+) respectively using Chinese semantic knowledge-base (E-HowNet).
%Now, if we use E-HowNet, the head words in (a) and (b) will generalize to one semantic type ?, and the target words to the semantic type ?. 
For our ten probabilistic models (see Section 4.2 for details about these models), Table 1 gives coverage and the corresponding accuracy statistics before and after adding these generalizations. As can be seen, after generalization, the coverage improved considerably for each feature combination, which ultimately resulted in better performance.
\begin{table}[!h]
\small
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
\multirow{2}{*}{\bf${P_f}_c$} & \multicolumn{2}{|l}{{\bf without E-HowNet}} & \multicolumn{2}{|l|}{\bf with E-HowNet} \\ \cline{2-5} 
                  &   Coverage       &      Accuracy     &     Coverage       &     Accuracy      \\ \hline
   1              &    7.10\%       &    23.61\%       &   12.68\%         &        28.00\%   \\ 
   2              &    52.06\%      &     59.14\%      &   73.05\%         &        74.98\% \\ 
   3              &    67.43\%      &     69.87\%      &   82.68\%         &        81.15\%   \\ 
   4              &    74.56\%       &     68.92\%     &   93.35\%          &        79.16\%  \\ 
   5              &    97.97\%      &      92.25\%     &   97.97\%         &         92.27\%  \\ 
   6              &    28.60\%       &     40.13\%      &  64.20\%          &        62.95\%   \\ 
   7              &    76.30\%       &     68.07\%      &  94.23\%          &        76.87\%  \\ 
   8              &    82.97\%       &      63.50\%     &  96.85\%          &        67.40\%  \\ 
   9              &    99.78\%       &      79.17\%     &   99.78\%         &        79.41\%   \\ 
   10             &    90.09\%       &      58.18\%     &   99.00\%         &          53.31\% \\ \hline
\end{tabular}
%\begin{tabular}{|c|c|c||c|c}
%\hline \bf ${P_f}_c$ & \multicolumn{2}{|c|}{\bf without E-HowNet}& \multicolumn{2}{|c|}{\bf with E-HowNet }\\
%\hline
% 1 & 7.10 & \% & 12.68\% & \\ 
% 2 & 52.06\% & 73.05\%   \\ 
% 3 & 67.43\% &  82.68\%   \\ 
% 4 & 74.56\% &  93.35\%  \\ 
% 5 & 97.97\% & 97.97\%   \\ 
% 6 & 28.60\% &  64.20\%   \\ 
% 7 & 76.30\% & 94.23\%   \\ 
% 8 & 82.97\% & 96.85\%   \\ 
% 9 & 99.78\% & 99.78\%   \\ 
% 10 & 90.09\% & 99.00\%   \\ 
%\hline 
%\end{tabular}  
\caption{Coverage \& Accuracy Statistics}
\end{center}
\end{table}
\normalsize
\subsection{Probabilistic Models}
Ten simple probabilistic models, each based on a particular feature combination, were build using the Sinical treebank labeled data. The probabilities were estimated using the following simple formula. 
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
  $P(r|constituent)$  
   $= P(r|f_{c})$
   $= #(r,f_{c})/#f_{c}$
\end{Verbatim} 
Where $f_c$ represents a particular feature combination. A number of combinations were tried, and for the final system we used the following set of ten combinations.
%\begin{verbatim}
\begin{verbatim}
{(semType_h_word,h_pos,
semType_t_word,t_pos,pt,position,
all_pos,passive,all_semType,
left_right_child_pos), 
(h_pos,semType_t_word,t_pos,pt,
position,passive),
(semType_h_word,h_pos,t_pos,pt,
position,passive),
(semType_t_word,t_pos,pt,passive,
position),
(h_pos,t_pos,pt,position,passive),
(semType_h_word,semType_t_word,
pt,position,passive),
(semType_t_word,t_pos,pt,passive),
(semType_t_word,t_pos,passive),
(t_pos,pt,position,passive),
(semType_t_word)}
\end{verbatim}
\subsection{Classification Method}
\subsubsection*{Notations}
\begin{itemize}
\item Let $F$ be the set of ten feature combinations (given in the previous section), and $f_c$ be an element of $F$
\item $P$ be the set of ten corresponding probabilistic models, and ${P_f}_c$ be an element of $P$ that is based on feature combination $f_c$ 
\item ${D_f}_c$ be the probability distribution computed by ${P_f}_c$ 
\item $M_{((p,r)|f_c)}$ be the most probable $(probability,role)$ pair in  ${D_f}_c$
\item $W$ be a set of optimal weights, $w$ be an element of $W$, and $wp$ be a weighted probability (i.e. rank)
\end{itemize} 
\subsubsection*{Algorithm}
Our classification algorithm is as follows:
\begin{enumerate}
\item For a test candidate, extract values of all features (mentioned in section 2) using the parse tree and E-HowNet.
\item initialize $potential\_roles$  $\leftarrow$ $empty$ 
\item For each $f_c$  $\in$ $F$ do:
\begin{itemize}
 \item Find probability distribution ${D_f}_c$ using the corresponding ${P_f}_c$ model
 \item Select $M_{((p,r)|f_c)}$ from ${D_f}_c$
 \item Rank $M_{((p,r)|f_c)}$ by multiplying $p$ with the corresponding $w$ from $W$
 \item Append $(wp,r)$ to $potential\_roles$
 \end{itemize}
\item Return the top ranked $r$ from $potential\_roles$
 \end{enumerate}
\subsection{An Example}
Suppose we want to assign a semantic role to the circled node of the parse tree given in Figure 1.
\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth,height=3.5cm]{./examples/example3-circled.png}
\caption{An example parsed sentence}
%\label{fig:example2-full}
\end{figure}
\noindent First, we have to extract the full set of \verb+(feature:value)+ pairs for the target test node. The extracted set is given below:
\begin{verbatim}
{(semType_h_word:advantage),
(h_pos:Nac),(semType_t_word:human),
(t_pos:NP??),(pt:NP),(position:-1),
(all_pos:NP??-NP-Neqa-Nab-DE),
(passive:False),
(left_right_child_pos:empty-Nac),
(all_semType:
BecomeMore-human-tool-advantage)}
\end{verbatim}
%semType\_h\_word:naming\\VG1,respective,NP,S,1,NP-VH11-VH11-VH11-DE-Nv1,True,VG1-empty,concession-name-tool-naming-black-white-3rdPerson-respective. 
The probability distribution estimated by each of the ten probabilistic models is given in Table\footnote{Note that due to sparseness of the training data, their is no probability distribution for the feature combination of model 1.} 2.
\begin{table}[!h]
\small
\begin{tabular}{|l|p{6.3cm}|}
\hline \bf ${P_f}_c$ & \bf  Probability Distribution (${D_f}_c$) \\ 
\hline 1 &  \\ 
\hline 2 & [(0.5072, 'possessor'), (0.4783, 'property'), 0.0145, 'quantifier')]\\ 
\hline 3 & [(0.5, 'property'), (0.5, 'possessor')] \\ 
\hline 4 & [(0.5035, 'property'), (0.4894, 'possessor'), (0.0071, 'quantifier')] \\ 
\hline 5 & [(0.8514, 'property'), (0.1361, 'possessor'), (0.0083, 'quantifier'), (0.0042, 'apposition')]\\ 
\hline 6 & [(0.5, 'property'), (0.5, 'possessor')]\\ 
\hline 7 & [(0.5035, 'property'), (0.4894, 'possessor'), (0.0071, 'quantifier')] \\ 
\hline 8 & [(0.5035, 'property'), (0.4894, 'possessor'), (0.0071, 'quantifier')]\\ 
\hline 9 &[(0.8598, 'property'), (0.1240, 'possessor'), (0.0132, 'quantifier'), (0.0015, 'apposition'), (0.0011, 'predication'), (0.0004, 'frequency')]\\ 
\hline 10 & [(0.1915, 'agent'), (0.1461, 'theme'), (0.1361, 'goal'), (0.1355, 'property'), (0.0984, 'range'), (0.0775, 'DUMMY'), (0.0540, 'possessor'), (0.0533, 'apposition'), (0.0414, 'experiencer'), (0.0267, 'DUMMY2'), (0.0230, 'DUMMY1'), (0.0073, 'topic'), (0.0024, 'location'), (0.0021, 'quantifier'), (0.0021, 'causer'), (0.0007, 'predication'), (0.0007, 'manner'), (0.0003, 'time'), (0.0003, 'particle'), (0.0002, 'complement'), (0.0002, 'comparison'), (0.0001, 'target'), (0.0001, 'source'), (0.0001, 'hypothesis'), (0.0001, 'companion')] \\ 
\hline 
\end{tabular}  
\caption{Probability distributions}
\end{table}
\noindent Next, from each distribution, we can select $M_{((p,r)|f_c)}$. The full list is given below:
\begin{verbatim}
[NULL,(0.5072,'possessor'),
(0.5,'property'),
(0.5035,'property'),
(0.8514,'property'),
(0.5,'property'),
(0.5035,'property'),
(0.5035,'property'),
(0.8598,'property'),
(0.1915,'agent')]
\end{verbatim}
Note that a \verb+NULL+ is inserted for the models that do not have any probability distribution.\\
Finally, each role in this list is ranked by multiplying its probability to the corresponding weight from $W$. These weights encode the worth of a particular feature combination in determining the semantic role, and we have used genetic algorithms and a held out development data-set to find the optimal set (i.e. $W$). The observed optimal set after 100 generations is given below:
\begin{verbatim}
{(w1:0.9),(w2:1.0),(w3:0.9),
(w4:0.7),(w5:0.8),(w6:0.4),
(w7:0.5),(w8:0.4),(w9:0.5),
(w10:0.4)} 
\end{verbatim}
The final list of ranked roles is:
\begin{verbatim}
[NULL,(0.5072,'possessor'),
(0.45,'property'),
(0.505307,'property'),
(0.68112,'property'),
(0.2,'property'),
(0.25175,'property'),
(0.2014,'property'),
(0.4299,'property'),
(0.0766,'agent')]
\end{verbatim}
The top ranked \verb+(probability,role)+ pair in this list is \verb+(0.68112,'property')+, hence the role \verb+'property'+ will be assigned to the test node by our classification method.
%\subsection{Discussion}
%A more constrained probabilistic model is supposed to be more reliable for correctness. This might be true in those cases where the role suggested by the more constrained model is also the more probable one. However, in the cases, where a less constrained model suggests a more probable role (as in the above example), there is a trade off between reliability and probability. One way to handle this particular scenario is suggested by our classification method, and that is to rank the roles by multiplying the probabilities to reliability weights and then select the top-ranked role. This approach worked better than the other classification approaches as can be noted from  the experimental results given in the next section.
%One can note that, in the above example, model 5, which is less constrained compared to model 2, suggested a more probable and correct role. This is where our model differed and outperformed the previously reported model. %However, one can argue that more constrained models are supposed to be more reliable. In our approach, this factor is encoded by weights, and we can see that models with more feature constraints tend to have higher weights, while it is the other way around for the less constrained models.  
\section{Experiments and Evaluation}
To evaluate the performance and to show the usefulness of our approach, we have build the following five semantic role labeling systems:
\begin{itemize}
\item \textbf{DT:} Based on Decision Tree classifier
\item \textbf{NB:} Based on Naive Bays classifier
\item \textbf{ME:} Based on Maximum Entropy classifier
\item \textbf{LI:} Based on simple probabilistic models together with linear interpolation
\item \textbf{WPM:} Based on our classification approach
\end{itemize} 
All of these systems were tested using the same feature set (given in Section 4.1), and the same training and testing data (i.e. Sinica Treebank). Results of a 10-fold cross validation scheme are given in Table 3. 
\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline \bf System & \bf Accuracy &  \bf Precision\\ \hline
 DT & 91.03\% & 91.04\%  \\ 
 NB & 92.58\% & 92.57\% \\ 
 ME & 92.67\% & 92.68\% \\ 
 LI & 94.26\% &  94.27\%\\ 
 WPM & 94.49\% & 94.49\% \\ 
\hline 
\end{tabular}
\caption{Evaluation results} 
\end{center}
\end{table}
From the results, we can see that our system outperformed all other systems, and linear interpolation based system is the most competitive one. \\
To further enhance the performance of our system, we added a post-processing component to fix some of the obvious mistakes made by the probabilistic models. One such case is disambiguation between possessor/property roles. We used a heuristic rule that if the semantic type of a target word is \verb+human+, it is more likely to be a \verb+possessor+ than a \verb+property+. With few other such rules, the scores were improved from 94.49\% to 94.73\%. 

\section{Conclusion}
We have presented an improved SRL system for Chinese. We have shown how by using a semantic knowledge-base to handle data sparseness, and by using a weighting strategy to rank the outcomes of simple probabilistic models, the performance of an existing system was enhanced. Together with gains through simple heuristic rules, the overall accuracy was improved from 92.71\% to 94.73\%.
% If you use BibTeX with a bib file named eacl2014.bib, 
% you should add the following two lines:
\clearpage
\bibliographystyle{acl}
\bibliography{eacl2014}

\end{document}
